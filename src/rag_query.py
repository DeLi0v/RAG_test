# src/rag_pipeline.py
import json
import faiss
import lmstudio as lms
from .embedder import LMStudioEmbedder
from .retriever import RetrieverWithRerank
from .utils import VECTOR_DIR, logger


class RAGPipeline:
    def __init__(self, k=5):
        self.embedder = LMStudioEmbedder()
        self.index = faiss.read_index(f"{VECTOR_DIR}/docs.index")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏
        self.docs = []
        with open(f"{VECTOR_DIR}/docs.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                self.docs.append(json.loads(line))

        self.k = k
        self.retriever = RetrieverWithRerank(self.index, self.docs)

    def search_docs(self, query):
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –≤–æ–ø—Ä–æ—Å–∞
        query_vec = self.embedder.embed(query)
        top_docs = self.retriever.retrieve(query_vec=query_vec, k=self.k, top_n=50)
        return top_docs

    def ask(self, query):
        context_docs = self.search_docs(query)
        if not context_docs:
            return "‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ."

        context_texts = [d["text"] for d in context_docs]
        context = "\n\n".join(context_texts)
        sources = list(dict.fromkeys(d["source"] for d in context_docs))

        prompt = f"""
            –ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}
            –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}
            """

        logger.info("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –ò–ò...")
        lms.set_sync_api_timeout(600)
        model = lms.llm("lmstudio-community/mistral-7b-instruct")
        response_stream = model.respond_stream(prompt, config={"temperature": 0.0})

        answer = ""
        for fragment in response_stream:
            chunk = fragment.content
            print(chunk, end="", flush=True)
            answer += chunk

        print(f"\nüìÑ –ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n" + "\n".join(sources))
        return answer
